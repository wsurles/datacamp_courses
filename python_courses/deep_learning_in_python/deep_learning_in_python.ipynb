{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Python\n",
    "- William Surles\n",
    "- 2017-12-22\n",
    "- Datacamp class\n",
    "- https://www.datacamp.com/courses/deep-learning-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basics of deep learning and neural networks**\n",
    "- Introduction to deep learning\n",
    "- Forward propagation\n",
    "- Activation functions\n",
    "- Deeper networks\n",
    "\n",
    "**Otimizing a neural network with backward propagation**\n",
    "- The need for optimization\n",
    "- Gradient descent\n",
    "- Backpropagation\n",
    "- Backpropagation in practice\n",
    "\n",
    "**Building deep learning models with keras**\n",
    "- Creating a keras model\n",
    "- Compiling and fitting a model\n",
    "- Classification models\n",
    "- Using models\n",
    "\n",
    "**Fine-tuning keras models**\n",
    "- Understanding model optimization\n",
    "- Model validation\n",
    "- Thinking about model capacity\n",
    "- Stepping up to images\n",
    "- Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Check this [wiki page](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research) on data sets for machine learning research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of deep learning and neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions\n",
    "- Imagine you work for a bank and you need to predict the number of transactions each customer will make next year\n",
    "- I linear regression model could give you an answer based on weighting certain factors, like age or number of kids. But it does not do a great job of capturing the intricate interactions of the real world process.\n",
    "- Neural networks account for interactions really well\n",
    "- Deep learning uses especially powerfyl neural networks\n",
    " - This works well for the intricacies in many types of data like...\n",
    " - Text, images, video, audio, source code, etc\n",
    " \n",
    "#### Course structure\n",
    "- First to chapters focus on conceptual knowledge\n",
    " - debug and tune deep learing models on conventional prediction problems\n",
    " - Lay the foundation for progressin towards modern applications\n",
    "- Chp 3 and 4 covers keras\n",
    " - Build and tune models in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are good pictures in the slides. I'll just explain it\n",
    "- Each link between nodes is given a weighting\n",
    "- the inputs times the weights added together equals the value at the next node\n",
    " - this is essentially a dot product of inputs and weights (yay matrix math)\n",
    "- keep doing this until you get the output value at the end. \n",
    "- This happens for one data point (or row, or observation) at a time. The output is the prediction for the data point\n",
    " - The trick is to get all the weightings correct so the model is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the forward propagation algorithm\n",
    "\n",
    "![](images/1_hidden_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "# Calculate node 0 value: node_0_value\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Well, that was easy. : ) \n",
    "- -39 seems like a wierd value but I'm sure will fix that later\n",
    "- This is just for one observation with two variables\n",
    "- And there is just one layer with 2 nodes.\n",
    "- About as simple as you can get. \n",
    "- But this is about as complex as most regression models I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation functions are applied at the node\n",
    "- The modify the node input value to something different for the node output\n",
    "- This is used to capture non-linearity in the the model\n",
    "- They have been shown to greatly increase the perofrmance of a neural network\n",
    "- For a long time `tanh()` was used as the standard activation function\n",
    "- Now ReLU (Rectified Linear Activation) is the standard\n",
    " - A ReLu function is basically just `max(0, input)`\n",
    " - Its prevents negative numbers at the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rectified Linear Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the network to many observations/rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights = {\n",
    "    'node_0': np.array([2, 4]), \n",
    "    'node_1': np.array([ 4, -5]), \n",
    "    'output': np.array([2, 7])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    \n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deeper networks just have more layers\n",
    "- 5- 10 maybe\n",
    "- At pone point 15 was the cutting edge\n",
    "- Now you could see 1000 hidden layers\n",
    "- Its just about proccessing power\n",
    " - These work well with parallel processing so you can scale out for the large networks\n",
    "- The more layers the more complex the features get\n",
    " - At first the model may find things like lines, then shapes, maybe even an eye\n",
    " - Then it may recognize a face \n",
    " - Then different faces like human or cat, or differnt people. \n",
    " - I'm sure this is how google and face book do face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer neural networks\n",
    "![](images/2_hidden_layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {\n",
    "    'node_0_0': np.array([2, 4]),\n",
    "    'node_0_1': np.array([ 4, -5]),\n",
    "    'node_1_0': np.array([-1,  2]),\n",
    "    'node_1_1': np.array([1, 2]),\n",
    "    'output': np.array([2, 7])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations are learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the weights that deterimine the features/interaction in Neural Networks created>\n",
    " - The model training process sets them to optimize predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizing a neural network with backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The weights are how you optimize the neural network. \n",
    "- Change the weights until the output is correct\n",
    "\n",
    "#### Predictions with multiple points\n",
    "- Making acurate predictions gets harder with more points\n",
    "- At any set of weights, there are many values of the error\n",
    "- ... corresponding to the many points we make predictions for\n",
    "\n",
    "#### Loss funciton\n",
    "- Aggregates errors in predictions from many data points into single number\n",
    "- Measure of model's predictive performance\n",
    "- Could be mean squarred error\n",
    "- Lower loss function value means a better model\n",
    "- Goal: Find the weights thta five the lowest value for the loss function\n",
    "- Gradient descent is how you find the lowest value\n",
    "\n",
    "#### Gradient descent\n",
    "- Imaginge you are in a pitch dark field\n",
    "- you want to find the lowest point\n",
    "- Feel the ground to see how it slopes\n",
    "- Take a small step downhill\n",
    "- Repeat until it is uphill in every direction\n",
    "\n",
    "#### Gradient descent steps\n",
    "- Start at random point\n",
    "- Until yo are somewhere flat:\n",
    " - Find the slope\n",
    " - Take a step downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding how weight changes affect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_network(input_data, weights):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_input = (input_data * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_1_input = (input_data * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "# The actual target value, used to calculate the error\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 0]\n",
    "            }\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = model_output_1 - target_actual\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up to multiple data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
    "target_actuals = [1, 3, 5, 7]\n",
    "\n",
    "weights_0 = {'node_0': [2, 1], \n",
    "             'node_1': [1, 2], \n",
    "             'output': [1, 1]}\n",
    "\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [ 1. ,  1.5],\n",
    "             'output': [ 1. ,  1.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 0, 12]\n",
      "[9.75, 10.0, 0.0, 14.0]\n",
      "target actuals:\n",
      "[1, 3, 5, 7]\n",
      "Mean squared error with weights_0: 37.500000\n",
      "Mean squared error with weights_1: 49.890625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_0\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "print(model_output_0)\n",
    "print(model_output_1)\n",
    "\n",
    "print('target actuals:')\n",
    "print(target_actuals)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the slope is positive:\n",
    " - Going opposite the slope means moving to lower numbers\n",
    " - Subtract the slope from the current value\n",
    " - Too big a step might lead us astray\n",
    "- Solution: learning rate\n",
    " - Update each weight by subtracting learning rate * slope\n",
    " - Learning rates are frequently around 0.01\n",
    "\n",
    "#### Slope calculation example\n",
    "- to calculate the slope for a weight, need to multiply 3 things:\n",
    " - slope of the loss function wrt value at the node we feed into\n",
    " - the valye of the node that feeds into our weight\n",
    " - slope of the activation functino wrt value we feed into\n",
    "- example\n",
    " - node = 3, weight = 2, output = 4, actual target value = 10\n",
    "- lets step through those 3 things:\n",
    "- slope of mean-squarred loss funciton wrt prediciton:\n",
    " - 2 * (Predictied Value = Actual Value) = 2 * Error = 2 * -4\n",
    "- the vale of the node that feeds into our weight\n",
    " - 2 * -4 * 3 = -24\n",
    "- slope of the activation functino wrt value we feed into\n",
    " - We don't have an activation function so ignore that here\n",
    "- if learning rate is 0.01 the new wieght would be \n",
    " - 2 - 0.01(-24) = 2.24\n",
    " - this will give us better model and will continue to get better as we repeat this process\n",
    " - We would repeat this process separately for each weight, then update them simultaneously with the respective deriviative\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When plotting the mean-squared error loss function against predictions, the slope is 2 * x * (y-xb), or 2 * input_data * error. \n",
    "- Note that x and b may have multiple numbers (x is a vector for each data point, and b is a vector). \n",
    "- In this case, the output will also be a vector, which is exactly what you want.\n",
    "- In our example we have 3 input nodes and one prediciton node as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "target: 0\n",
      "[14 28 42]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "print(\"pred:\", preds)\n",
    "print(\"target:\", target)\n",
    "\n",
    "# Calculate the error: error\n",
    "error =  preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * error * input_data\n",
    "\n",
    "# Print the slope\n",
    "print(slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - (slope * learning_rate)\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making multiple updates to weights\n",
    "- Lets clean up this process so we can do this multiple times easily\n",
    "- We are wrapping the error and slope calculations in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(input_data, target, weights):\n",
    "    preds = (weights * input_data).sum()\n",
    "    error = preds - target\n",
    "    return(error)\n",
    "    \n",
    "def get_slope(input_data, target, weights):\n",
    "    error = get_error(input_data, target, weights)\n",
    "    slope = 2 * input_data * error\n",
    "    return(slope)\n",
    "\n",
    "def get_mse(input_data, target, weights):\n",
    "    errors = get_error(input_data, target, weights)\n",
    "    mse = np.mean(errors**2)\n",
    "    return(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0f4d7048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - slope * 0.01\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "- Comes from chain rule of calculus\n",
    "- Important to understand the process, but you will generally use a library that implements this\n",
    "\n",
    "#### Backpropagation process\n",
    "- Trying to estimate the slope of the loss function wrt each weight\n",
    "- Do forward propagation to calculate predictions and errors first, before we do backpropagation\n",
    "- Go back one layer at a time\n",
    "- Gradients for weight is product of:\n",
    "  - Node value feeding into that weight\n",
    "    - We know this from the calculation when doing forward propagation\n",
    "  - Slope of loss function wrt node it feeds into\n",
    "    - We just calculated this in the back propagation process. \n",
    "  - Slope of activation function at the node it feeds into\n",
    "    - For the ReLU activation function this is 0 if the value is negative and 1 if its positive\n",
    "- We need to also keep track of the slopes of the loss funciton wrt node values\n",
    "  - Slope of node values are just the sum of the slopes for all weights that come out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relationship between forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- its 1 to 1\n",
    "- Each time you generate predictions using forward propagation, you updated the weights using backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- If your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. \n",
    "- In that circumstance, the updates to all weights in the newtwork would also be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- See a simple example of backpropagtion in the slides\n",
    "\n",
    "#### Calculating slopes associated with any weight\n",
    "- Gradients for weight is product of:\n",
    "  - Node value feeding into that weight\n",
    "  - Slope of activation function for the node being fed into \n",
    "    - This is simple the sum of slopes of weights coming out of node\n",
    "  - Slope of loss function wrt output node\n",
    "    - For relu this is 0 if negative and 1 if positive value at node\n",
    "\n",
    "#### Backpropagation: Recap\n",
    "- Start at some random set of weights\n",
    "- Use forward propagation to make a prediction\n",
    "- Use backward propagation to calculate the slope of the loss function wrt each weight\n",
    "- Multiply that slope by the learning rate, and subtract from the current weights\n",
    "- Keep going with that cycle until we get to a flat part\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "- It is common to calclate slopes on only a subset of the data ('batch')\n",
    "- Use a different batch of data to calculate the next update\n",
    "- Start over from the beginning once all data is used\n",
    "- Each time through the training data is called an epoch\n",
    "- When slopes are calculated on one batch at a time its called stochastic gradient descent\n",
    "- Note: I guess this is a way to prevent over fitting to a specific 'batch' of training data. Or it could just be used to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building deep learning models with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model building steps\n",
    "- Specify Architecture\n",
    "- Compile\n",
    "- Fit\n",
    "- Predict\n",
    "\n",
    "#### keras notes\n",
    "- you need to specify the number of inputs to the first layer. This is the number of features or columns in the data\n",
    "- We will focus on sequential. This means layers connect only to the next layer. Like what we have practiced in this clas. There are more complex layer formats but this will work for now. \n",
    "- In a dense layer all of the nodes in the previous layer connect to all the nodes in the current layer.\n",
    "- Here we use the 'relu' activation function. Keras supports pretty much all activtion functions we would want to use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'https://assets.datacamp.com/production/course_1975/datasets/hourly_wages.csv'\n",
    "wages = pd.read_csv(file)\n",
    "wages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  5.1 ,   4.95,   6.67,   4.  ,   7.5 ,  13.07])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wages['wage_per_hour'].values\n",
    "print(target.shape)\n",
    "target[0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  8, 21, ...,  0,  1,  0],\n",
       "       [ 0,  9, 42, ...,  0,  1,  0],\n",
       "       [ 0, 12,  1, ...,  0,  1,  0],\n",
       "       ..., \n",
       "       [ 1, 17, 25, ...,  0,  0,  0],\n",
       "       [ 1, 12, 13, ...,  1,  0,  0],\n",
       "       [ 0, 16, 33, ...,  0,  1,  0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = wages.drop('wage_per_hour', axis = 1).values\n",
    "print(predictors.shape)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation = 'relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Compile your model\n",
    "- Specify the optimizer\n",
    "  - Controls the learning rate\n",
    "  - Many options and mathematically complex\n",
    "    - Even the experts don't know all the details and options\n",
    "  - \"Adam\" is ususally a good robust choice\n",
    "- Loss function\n",
    "  - \"mean_squarred_error\" is common for regression problems\n",
    "  - we will use something else later for classificaiton problems\n",
    " \n",
    "#### Fitting a model\n",
    "- Applying backpropation and gradient descent with your data to update the weights\n",
    "- This will be similar to what we did in scikitlearn but with more options\n",
    "- Scaling data before fitting can ease optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [documention](https://keras.io/optimizers/#adam) on \"adam\" model and others\n",
    "- [original paper](https://arxiv.org/abs/1412.6980v8) on adam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "534/534 [==============================] - 0s - loss: 23.9523     \n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s - loss: 22.2524     \n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s - loss: 23.1267     \n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s - loss: 22.4713     \n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s - loss: 21.8556     \n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s - loss: 21.5578     \n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s - loss: 20.8700     \n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s - loss: 21.9230     \n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s - loss: 21.6118     \n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s - loss: 20.6129     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2288dfd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classification\n",
    "- 'categorical_crossentropy' loss function\n",
    "  - Not the only one, but by far the most common\n",
    "- Similar to log loss: lower to better\n",
    "- Add metrics = ['accuracy'] to compile step for easy-to-understand diagnostics\n",
    "- Output layer has separate node for each possible outcome and uses 'softmax' activation\n",
    "  - this is ensures the output sums to 1 so they can be interpreted as probabilities\n",
    "\n",
    "- We will have a separate node in the output for each possible class\n",
    "- We will split or results classification into a boolean column for each outcome\n",
    "  - This is called one hot encoding\n",
    "  \n",
    "- We use the function `to_categorical` to do the one hot encoding\n",
    "- the instructor likes to load data with pandas so he can inspect it easily. I do too.\n",
    "- We will drop the target and then convert to a numpy matrix with `to_matrix`\n",
    "- Then convert the target data to categorical columns\n",
    "- After that our model looks the same except we use 2 nodes at the end andit has the softmax activation function\n",
    "- We will look at better ways to determine how long to train later on.\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1            False   \n",
       "1         1       1  38.0      1      0  71.2833     0            False   \n",
       "2         1       3  26.0      0      0   7.9250     0            False   \n",
       "3         1       1  35.0      1      0  53.1000     0            False   \n",
       "4         0       3  35.0      0      0   8.0500     1            False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'https://assets.datacamp.com/production/course_1975/datasets/titanic_all_numeric.csv'\n",
    "titanic = pd.read_csv(file)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.002015</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208   \n",
       "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429   \n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400   \n",
       "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200   \n",
       "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000   \n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200   \n",
       "\n",
       "             male  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "count  891.000000               891.000000                891.000000   \n",
       "mean     0.647587                 0.188552                  0.086420   \n",
       "std      0.477990                 0.391372                  0.281141   \n",
       "min      0.000000                 0.000000                  0.000000   \n",
       "25%      0.000000                 0.000000                  0.000000   \n",
       "50%      1.000000                 0.000000                  0.000000   \n",
       "75%      1.000000                 0.000000                  0.000000   \n",
       "max      1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       embarked_from_southampton  \n",
       "count                 891.000000  \n",
       "mean                    0.722783  \n",
       "std                     0.447876  \n",
       "min                     0.000000  \n",
       "25%                     0.000000  \n",
       "50%                     1.000000  \n",
       "75%                     1.000000  \n",
       "max                     1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last steps in classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 22.0, 1, ..., 0, 0, 1],\n",
       "       [1, 38.0, 1, ..., 1, 0, 0],\n",
       "       [3, 26.0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [3, 29.69911764705882, 1, ..., 0, 0, 1],\n",
       "       [1, 26.0, 0, ..., 1, 0, 0],\n",
       "       [3, 32.0, 0, ..., 0, 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = titanic.drop('survived', axis = 1).values\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(titanic.survived)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.survived.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 3.7363 - acc: 0.5915     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 2.4379 - acc: 0.6386     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 1.6070 - acc: 0.6251     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.8699 - acc: 0.6083     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6500 - acc: 0.6790     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6460 - acc: 0.6768     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.6203 - acc: 0.6756     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5954 - acc: 0.6857     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5809 - acc: 0.7026     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5879 - acc: 0.7037     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a22b72908>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation = 'relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = 'sgd',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What if I used more layers.\n",
    "- Just doing a litle practice here\n",
    "- The example in the slides had more layers and an `adam` optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 1.2053 - acc: 0.6139     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.6568 - acc: 0.6712     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.6202 - acc: 0.6689     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6006 - acc: 0.7015     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6223 - acc: 0.6835     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6002 - acc: 0.6970     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5935 - acc: 0.7116     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5977 - acc: 0.7116     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5982 - acc: 0.6970     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5915 - acc: 0.7026     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a22d99710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'sgd',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using a thousand nodes in each layer makes it way slower, but it gets more accurate\n",
    "- Wait how does it test the accuracy? Does it use cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 1.3850 - acc: 0.6364     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.6407 - acc: 0.6599     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.6035 - acc: 0.6835     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6009 - acc: 0.6857     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.5830 - acc: 0.7093     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.5498 - acc: 0.7217     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5435 - acc: 0.7340     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5237 - acc: 0.7542     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5092 - acc: 0.7744     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5113 - acc: 0.7778     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a23092c18>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Steps \n",
    "- Save the model\n",
    "- Reload it\n",
    "- Make predictions with it\n",
    "\n",
    "#### Notes\n",
    "- Use the save method and a file name to save the model \n",
    "- we use hdf5 files to save the model which are .h5 extension\n",
    "- The predictions result will be in the same format as the target variable (two columns, categorical)\n",
    " - We just want percent survived here so we take the second column\n",
    " - I'm using the predictors data that we used to train the model just for the example\n",
    " - Of course I'd want to use testing data normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('model_file.h5')\n",
    "predictions = my_model.predict(predictors)\n",
    "probability_true = predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1120833 ,  0.83139241,  0.49863389,  0.75253439,  0.1349951 ,\n",
       "        0.21149245,  0.54944605,  0.17865145,  0.47126898,  0.66157144], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_true[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1000)              11000     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 2,015,002\n",
      "Trainable params: 2,015,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "- This just repeats what I did above, but this is the full process\n",
    "- Except I don't have hold out data so I will use the predictors matrix again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 1.7703 - acc: 0.6240     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 1.0856 - acc: 0.6296     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.8507 - acc: 0.6611     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6627 - acc: 0.6779     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6611 - acc: 0.6869     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6244 - acc: 0.6745     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5952 - acc: 0.6869     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5894 - acc: 0.6902     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5776 - acc: 0.7037     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5874 - acc: 0.6824     \n",
      "[ 0.3534275   0.63720858  0.53274614  0.60081106  0.35978973  0.42827499\n",
      "  0.51305556  0.39906359  0.69578516  0.53771579  0.47262841  0.78592378\n",
      "  0.41851735  0.3808707   0.53381234  0.59713489  0.39880708  0.510961\n",
      "  0.57460928  0.50815487]\n"
     ]
    }
   ],
   "source": [
    "# Specify, compile, and fit the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(predictors)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Why optimization is hard\n",
    "- Simultanesously optimixing 1000s of parameters with complex relationships\n",
    "  - The optimum weight for any one weight depends on the values of the other weights and there are a lot of weights\n",
    "- Updates may not improve model meaningfully\n",
    "- Updates too small (if learning rate is low) or too large (if learning rate is high)\n",
    "  - Smart optimizers like `adam` help, but problems can still occur\n",
    "\n",
    "#### Dying neuron problem\n",
    "- Once a node starts always getting negative inputs\n",
    "  - It may continue only getting negative inputs\n",
    "- Conributes nothing to the model\n",
    "  - \"Dead\" neuron\n",
    "\n",
    "#### Vanishing gradient problem\n",
    "- Occurs when many layers have very small slopes (e.g. due to being on flat par of tanh curve)\n",
    "- This can happen when using tanh activation funciton. There is no zero slope. But as the values get high the slopes approch zero\n",
    "- In deep networks, updates to backprop were close to 0\n",
    "- Research on activation functions is on going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 3.1818     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 3.1507     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 3.1194     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 3.0883     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 3.0573     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 3.0266     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 2.9961     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 2.9657     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 2.9356     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 2.9057     \n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 2.3632     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.6984     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.6477     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6272     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6094     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6059     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.6025     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.6064     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.6000     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.6019     \n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 9.5850     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \n"
     ]
    }
   ],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "input_shape = (predictors.shape[1],)\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model(input_shape)\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = my_optimizer, \n",
    "        loss = 'categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Validation in deep learning\n",
    "- Commonly use validation split rather than cross-validation\n",
    "- Deep learning widely used on large datasets\n",
    "- Single validation score is based on large amount of data, and is reliable\n",
    "- Repeated training from cross-validation would take long time\n",
    "\n",
    "#### Model Validation\n",
    "- You can use `validation_split = 0.3` in the `model.fit` function\n",
    "\n",
    "#### Earling Stopping\n",
    "- You can set up an early stopping monitor with a patience. \n",
    "- The patients is the number of epochs that the model can go withouth improving before it stops\n",
    "- One epoch is normal, but 2 or 3 is unlikely. If that many epochs go by without improvement the model is uunliekly to turn around and start improving again. \n",
    "- The early stopping monitor is passed into the `model.fit` function as a callback. \n",
    "- There are other callbacks we can used when we get more advanced at this.\n",
    "- Now we can set a higher max epochs because we know our model will stop if it finished early\n",
    "  - we use the `epochs = 20` argument\n",
    "  \n",
    "#### Experimentation\n",
    "- Experiment with differnt architectures\n",
    "- More layers, fewer layers, more nodes, fewer nodes, etc\n",
    "- A good model requires some experimentation\n",
    "- We will need to build more intuition on where to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model accuracy on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/10\n",
      "623/623 [==============================] - 0s - loss: 1.1801 - acc: 0.5907 - val_loss: 0.5950 - val_acc: 0.7201\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 0s - loss: 0.7189 - acc: 0.6260 - val_loss: 0.8434 - val_acc: 0.6418\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 0s - loss: 0.7093 - acc: 0.6308 - val_loss: 0.5980 - val_acc: 0.7537\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 0s - loss: 0.6414 - acc: 0.6854 - val_loss: 0.6069 - val_acc: 0.7164\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 0s - loss: 0.6204 - acc: 0.6822 - val_loss: 0.6076 - val_acc: 0.7127\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - 0s - loss: 0.6050 - acc: 0.6902 - val_loss: 0.8333 - val_acc: 0.6418\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - 0s - loss: 0.6338 - acc: 0.6613 - val_loss: 0.6267 - val_acc: 0.6866\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - 0s - loss: 0.5822 - acc: 0.6982 - val_loss: 0.5007 - val_acc: 0.7313\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - 0s - loss: 0.5614 - acc: 0.7191 - val_loss: 0.5747 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - 0s - loss: 0.6705 - acc: 0.6790 - val_loss: 0.4853 - val_acc: 0.7463\n"
     ]
    }
   ],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(\n",
    "    predictors, target, \n",
    "    validation_split = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping: Optimizing the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/30\n",
      "623/623 [==============================] - 0s - loss: 1.1611 - acc: 0.6340 - val_loss: 0.6281 - val_acc: 0.7164\n",
      "Epoch 2/30\n",
      "623/623 [==============================] - 0s - loss: 0.7219 - acc: 0.6453 - val_loss: 0.5069 - val_acc: 0.7537\n",
      "Epoch 3/30\n",
      "623/623 [==============================] - 0s - loss: 0.7429 - acc: 0.6485 - val_loss: 0.6091 - val_acc: 0.7201\n",
      "Epoch 4/30\n",
      "623/623 [==============================] - 0s - loss: 0.6417 - acc: 0.6661 - val_loss: 0.4943 - val_acc: 0.7537\n",
      "Epoch 5/30\n",
      "623/623 [==============================] - 0s - loss: 0.5877 - acc: 0.6870 - val_loss: 0.4948 - val_acc: 0.7463\n",
      "Epoch 6/30\n",
      "623/623 [==============================] - 0s - loss: 0.5868 - acc: 0.6838 - val_loss: 0.4926 - val_acc: 0.7500\n",
      "Epoch 7/30\n",
      "623/623 [==============================] - 0s - loss: 0.5792 - acc: 0.7095 - val_loss: 0.5859 - val_acc: 0.6679\n",
      "Epoch 8/30\n",
      "623/623 [==============================] - 0s - loss: 0.5615 - acc: 0.7095 - val_loss: 0.5402 - val_acc: 0.7761\n",
      "Epoch 9/30\n",
      "623/623 [==============================] - 0s - loss: 0.5839 - acc: 0.7127 - val_loss: 0.5158 - val_acc: 0.7873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a250a3fd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    predictors, target,\n",
    "    epochs = 30,\n",
    "    validation_split = 0.3,\n",
    "    callbacks = [early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This seems a little confusing to me still. \n",
    "- The `val_loss` score hits .5130. The next 2 epochs are not as good but it does not stop there\n",
    "- Its stops later after one non improving epoch. Seems to make no sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with wider networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2552deb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create model 1\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(10, activation='relu', input_shape = input_shape))\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_1.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(\n",
    "    predictors, target, \n",
    "    epochs=15, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping_monitor], \n",
    "    verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(\n",
    "    predictors, target, \n",
    "    epochs=15, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping_monitor], \n",
    "    verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(\n",
    "    model_1_training.history['val_loss'], 'r', \n",
    "    model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The blue model is very different each time I run this code chunk\n",
    "  - It does not seem as stable. It has a tendency to jump up and down\n",
    "- The red model usually looks fairly similar on each run.\n",
    " - Some times it actually does better than the blue model which is supposed to be better here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers to a network\n",
    "- Now we'll experiment with a deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a26c17f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create model 1\n",
    "# Create the new model: model_2\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "model_1.add(Dense(50, activation='relu'))\n",
    "model_1.add(Dense(50, activation='relu'))\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_1\n",
    "model_1.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(\n",
    "    predictors, target, \n",
    "    epochs=20, \n",
    "    validation_split=0.4, \n",
    "    callbacks=[early_stopping_monitor], \n",
    "    verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(\n",
    "    predictors, target, \n",
    "    epochs=20, \n",
    "    validation_split=0.4, \n",
    "    callbacks=[early_stopping_monitor], \n",
    "    verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(\n",
    "    model_1_training.history['val_loss'], 'r',\n",
    "    model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is an interesting example.  But again it seems to be a coin flip as to which model is better. \n",
    "- Also they pth seem to jump up in the validation score sometimes at random. \n",
    "  - I would think the tuning would improve over time steadily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Its still a little more of an 'art' to tune good deep learning algorithms compared to other types of maching learning algorithms\n",
    "- Model capacity should be one of the key considerations when considering what models to try \n",
    "- Model (or network) capacity is closely related to over and under fitting\n",
    "- Model capicity is our models ability to capture predictive patterns in our data\n",
    "- Adding more nodoes to a layer or more layers, increases the models capacity\n",
    "![](images/model_capacity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow for optimizing model capacity\n",
    "- Start with a small network\n",
    "- Gradually increase capacity\n",
    "- Keep increasing capacity until validation score is no longer improving\n",
    "- You can back it up a little bit but the model is probably near the ideal\n",
    "- Here is an example workflow\n",
    "![](images/workflow.png)\n",
    "\n",
    "\n",
    "- How should we add capaicty, nodes or layer?\n",
    "  - There is no universal answer to this\n",
    "  - Generally we just want to consider if we are increasing or decreasing capacity\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping up to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Recognizin handwritten digits\n",
    "- MNIST dataset\n",
    "  - This is a very popular dataset to get started working with images\n",
    "  - I think we used this in Andrew Ng's machine learning class on coursera\n",
    "- 28 x 28 grid flattened to 784 values for each image\n",
    "- Value in each part of array denotes darkness of that pixel 0 - 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own digit recognition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've loaded only 2500 images, rather than 60000 which you will see in some published results. \n",
    "- Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\n",
    "\n",
    "- If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. \n",
    "- If you don't have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. \n",
    "- Here is a [blog post](https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws) by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.608</th>\n",
       "      <th>0.609</th>\n",
       "      <th>0.610</th>\n",
       "      <th>0.611</th>\n",
       "      <th>0.612</th>\n",
       "      <th>0.613</th>\n",
       "      <th>0.614</th>\n",
       "      <th>0.615</th>\n",
       "      <th>0.616</th>\n",
       "      <th>0.617</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...    0.608  0.609  0.610  \\\n",
       "0  4  0    0    0    0    0    0    0    0    0  ...        0      0      0   \n",
       "1  3  0    0    0    0    0    0    0    0    0  ...        0      0      0   \n",
       "2  0  0    0    0    0    0    0    0    0    0  ...        0      0      0   \n",
       "3  2  0    0    0    0    0    0    0    0    0  ...        0      0      0   \n",
       "4  8  0    0    0    0    0    0    0    0    0  ...        0      0      0   \n",
       "\n",
       "   0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
       "0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'https://assets.datacamp.com/production/course_1975/datasets/mnist.csv'\n",
    "digits = pd.read_csv(file)\n",
    "print(digits.shape)\n",
    "digits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = digits.drop('5', axis = 1).values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(digits.iloc[:,0])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm actyally just getting around 53% accuracy so I'm going to tweek this a bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a287fdc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a287fdc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model_training = model.fit(\n",
    "    X, y, \n",
    "    epochs = 20,\n",
    "    validation_split = 0.3,\n",
    "    callbacks = [EarlyStopping(patience = 3)],\n",
    "    verbose = False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_training.history['val_acc'], 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Well that a little dissappointing\n",
    "- I was hoping for something in the high nineties\n",
    "- In the class we get .8893 with what I think is the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The instructor says its just like riding a bike. The hardest part is getting to the point where you can practice on your own. And now we are there\n",
    "- Start with problems that involve standard prediction problems on tables of numbers\n",
    "  - So like the ones we have seen in previous classes\n",
    "  - I can also use kaggle or other tutorials to see how this will perform\n",
    "- Then try images (with convolutional neural networks)\n",
    "- Kaggle is a great place to find datasets to work with\n",
    " - And there forums are a good place to keep learning as well\n",
    "- Check this [wiki page](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research) on data sets for machine learning research\n",
    "- keras.io has excellent documention\n",
    " - it also has nice examples\n",
    "- tensor flow also has some good examples\n",
    "- We need to set up a computer with GPUs. Get some on amazon. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_datacamp]",
   "language": "python",
   "name": "conda-env-python_datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
